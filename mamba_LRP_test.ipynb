{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4b58e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "import argparse\n",
    "import torch\n",
    "print(torch.__version__, torch.version.cuda)\n",
    "from models_mamba_original import vim_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2\n",
    "from mamba_lrp.model.vision_mamba import ModifiedVisionMamba\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0128ec3c",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = './CUB/DATASET/CUB_200_2011'\n",
    "DATASET_VALIDATION_RANDOM_SEED = 123\n",
    "BATCH_SIZE = 8\n",
    "DATASET_WORK_NUMBER = 8\n",
    "DATASET_SPLIT_RATIO = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f6fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetLoader.cub import CUB\n",
    "\n",
    "trans_train = transforms.Compose([\n",
    "    # A SCELTA NOSTRA AUGUMENTATION\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # transforms.RandomRotation(30),\n",
    "    \n",
    "    # ATTENZIONE QUA NON DOVREBBE SERIVIRE IL RESIZE (?)\n",
    "    # transforms.RandomResizedCrop(224, scale=(0.7, 1), ratio=(3/4, 4/3)),\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "trans_test = transforms.Compose([\n",
    "    # ATTENZIONE QUA NON DOVREBBE SERIVIRE IL RESIZE (?)\n",
    "    transforms.Resize((224, 224)),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# create dataset\n",
    "train_data = CUB(DATASET_ROOT, 'train', DATASET_SPLIT_RATIO, DATASET_VALIDATION_RANDOM_SEED, transform=trans_train)\n",
    "valid_data = CUB(DATASET_ROOT, 'valid', DATASET_SPLIT_RATIO, DATASET_VALIDATION_RANDOM_SEED, transform=trans_test)\n",
    "test_data = CUB(DATASET_ROOT, 'test', 0, 0, transform=trans_test)\n",
    "\n",
    "print(\"Train: {}\".format(len(train_data)))\n",
    "print(\"Valid: {}\".format(len(valid_data)))\n",
    "print(\"Test: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac59b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def imshow(image, label, ax=None, normalize=True):\n",
    "    \"\"\"show single along with label on an ax\"\"\"\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "    ax.set_title(label)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def show_samples(images, labels, nrows=2, ncols=3, title=None, normalize=True):\n",
    "    \"\"\" show multiple samples\n",
    "\n",
    "    args:\n",
    "        nrows (int, optional): number of row\n",
    "        ncols (int, optional): number of column\n",
    "        title (str, optional): title.\n",
    "        normalize (bool, optional): whether the images are normalized\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows, ncols, facecolor='#ffffff', dpi=100)\n",
    "\n",
    "    # .flat: to map samples to multi-dimensional axes\n",
    "    for (ax, image, label) in zip(axes.flat, images, labels):\n",
    "        ax = imshow(image, label, ax, normalize)\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout = True\n",
    "    fig.subplots_adjust(top=0.85, hspace=0.3)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7869e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show samples\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "images, labels, _ = next(iter(train_loader))\n",
    "show_samples(images[0:6], labels[0:6], 2, 3, 'SHOW SOME SAMPLES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03edb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=False, num_workers=DATASET_WORK_NUMBER)\n",
    "valid_loader = data.DataLoader(valid_data, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=DATASET_WORK_NUMBER)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=DATASET_WORK_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e2dd5b",
   "metadata": {},
   "source": [
    "# Model creation (work in progress)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975fdd1f",
   "metadata": {},
   "source": [
    "#### parameters that were removed from previous versions of Mamba-ssm (and default values):\n",
    "- bimamba_type (='v2' in VisionMamba, =False in Mamba-ssm)\n",
    "- if_divide_out (=False)\n",
    "- init_layer_scale (=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0fbdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import list_repo_files\n",
    "from einops import repeat\n",
    "files = list_repo_files(\"hustvl/Vim-tiny-midclstok\")\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899e1c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = vim_tiny_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_midclstok_div2(pretrained=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6fe35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#checkpoint_path = hf_hub_download(repo_id=\"hustvl/Vim-tiny-midclstok\", filename=\"vim_t_midclstok_76p1acc.pth\")\n",
    "\n",
    "torch.serialization.add_safe_globals([argparse.Namespace])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "# Load the model's weights\n",
    "\n",
    "#checkpoint = torch.load(checkpoint_path)\n",
    "#status = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "\n",
    "#freeze parameters\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "#modify head for 200 classes instead of original 1000\n",
    "model.head = torch.nn.Linear(model.head.in_features, 200)\n",
    "\n",
    "print(device)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "modified_model = ModifiedVisionMamba(model, zero_bias=True)\n",
    "modified_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d9d6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, verbose=False, delta=0):\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = float('inf')\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.val_loss_min = val_loss\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.verbose:\n",
    "                print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.val_loss_min = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b9e9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "optimizer = optim.SGD(modified_model.parameters(), lr=0.001, momentum=0.9, weight_decay=5e-4)\n",
    "\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.8)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=5, verbose=True)\n",
    "num_epochs = 30\n",
    "train_losses, val_losses = [], []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2324b10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "\n",
    "    for images, labels, _ in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_running_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in valid_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = loss_function(outputs, labels)\n",
    "            val_running_loss += loss.item()\n",
    "    val_loss = val_running_loss / len(valid_loader)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Print the losses for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] → Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    # Step the learning rate scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    early_stopping(val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(f\"Early stopping at epoch {epoch+1}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ce5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = 'mamba_cub_finetuned.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c854020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#salvataggio pesi modello\n",
    "torch.save(modified_model.state_dict(), checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5486fa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#caricamento pesi modello\n",
    "checkpoint_fine_tuned = torch.load(checkpoint_path)\n",
    "status = modified_model.load_state_dict(checkpoint_fine_tuned, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7177eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import visualization as viz\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb8cfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mamba_lrp.lrp.utils import vision_relevance_propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22c44b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from scipy.spatial import distance\n",
    "from scipy.stats import pearsonr, chisquare\n",
    "import torchvision.transforms.functional as TF\n",
    "import os\n",
    "import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcb515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate distributions from heatmaps (with resize if necessary)\n",
    "def saliency_to_distribution(saliency_map, target_size=None, eps=1e-12):\n",
    "  \n",
    "    # Convert to np.array\n",
    "    if isinstance(saliency_map, Image.Image):\n",
    "        # PIL image\n",
    "        array = np.array(saliency_map, dtype=np.float64)\n",
    "    elif isinstance(saliency_map, torch.Tensor):\n",
    "        # PyTorch tensor\n",
    "        array = saliency_map.detach().cpu().numpy().astype(np.float64)\n",
    "    elif isinstance(saliency_map, np.ndarray):\n",
    "        array = saliency_map.astype(np.float64)\n",
    "    else:\n",
    "        raise TypeError(\"The input must be a PIL Image, np.ndarray, or torch.Tensor\")\n",
    "    \n",
    "    # We make sure it's 2D\n",
    "    array = np.squeeze(array)\n",
    "    if array.ndim != 2:\n",
    "        raise ValueError(f\"Input map must be 2D or convertible to 2D. Input shape: {saliency_map.shape}\")\n",
    "\n",
    "    # Resize\n",
    "    if target_size is not None:\n",
    "        # target_size is (H, W), PIL.resize needs (W, H)\n",
    "        pil_target_size = (target_size[1], target_size[0]) \n",
    "        if array.shape != target_size:\n",
    "            # Conversion to PIL image for a more consistent resize\n",
    "            pil_img = Image.fromarray(array)\n",
    "            # Bilinear interpolation \n",
    "            pil_img = pil_img.resize(pil_target_size, Image.BILINEAR) \n",
    "            array = np.array(pil_img, dtype=np.float64)\n",
    "    \n",
    "    # Computing distribution\n",
    "    # No negative values\n",
    "    array = np.clip(array, a_min=0, a_max=None)\n",
    "    \n",
    "    dist = (array + eps).ravel()\n",
    "    \n",
    "    # Normalization\n",
    "    dist_sum = dist.sum()\n",
    "    if dist_sum < 1e-9: \n",
    "        warnings.warn(\"Saliency map near 0. Returning a uniform distribution.\")\n",
    "        dist = np.ones_like(dist) / len(dist) \n",
    "    else:\n",
    "        dist = dist / dist_sum\n",
    "            \n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f2f468a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GazeMap path\n",
    "GAZE_MAP_DIR = 'CUB/GAZE_DATASET/CUB_GHA'\n",
    "\n",
    "# Scores lists\n",
    "all_jss_scores = []\n",
    "all_chi2_scores = []\n",
    "all_pcc_scores = []\n",
    "\n",
    "print(\"Computing similarities metrics:\")\n",
    "\n",
    "model.patch_embed.requires_grad = False\n",
    "pretrained_embeddings = model.patch_embed\n",
    "#with torch.enable_grad():\n",
    "    \n",
    "for images, labels, image_ids in test_loader:\n",
    "    \n",
    "    images = images.to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Obtaining heatmap\n",
    "    embeddings = pretrained_embeddings(images)\n",
    "    R, prediction, logits = vision_relevance_propagation(\n",
    "        model = modified_model,\n",
    "        embeddings = embeddings,\n",
    "        targets = labels,\n",
    "        n_classes = 200\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        num_patches_side = int(np.sqrt(R.shape[1]))\n",
    "        model_heatmaps_batch = R[:,:-1].reshape(-1, num_patches_side, num_patches_side)\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping batch. Error: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Loop over all the batch images\n",
    "    for i in range(images.shape[0]):\n",
    "        \n",
    "        current_image_id = image_ids[i].item() \n",
    "        \n",
    "        model_map_tensor = model_heatmaps_batch[i]\n",
    "        \n",
    "        # Loading the \"Ground Truth\"\n",
    "        gaze_map_path = os.path.join(GAZE_MAP_DIR, f\"{current_image_id}.jpg\")\n",
    "        \n",
    "        if not os.path.exists(gaze_map_path):\n",
    "            warnings.warn(f\"Gaze Map File not found, skipping: {gaze_map_path}\")\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            gt_img = Image.open(gaze_map_path).convert(\"L\")\n",
    "            gt_target_size = (gt_img.height, gt_img.width) \n",
    "            \n",
    "            # Computing P and Q\n",
    "            Q = saliency_to_distribution(gt_img, target_size=None) \n",
    "            P = saliency_to_distribution(model_map_tensor, target_size=gt_target_size)\n",
    "            \n",
    "            # Computing the metrics\n",
    "            jsd = jensenshannon(P, Q)\n",
    "            all_jss_scores.append(1 - jsd)\n",
    "            all_chi2_scores.append(chisquare(P, Q))\n",
    "            all_pcc_scores.append(pearsonr(P, Q)[0]) # Only the coefficient\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in image {current_image_id}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86a6fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average scores\n",
    "if all_jss_scores:\n",
    "    print(\"\\n--- Average Final Results ---\")\n",
    "    print(f\"Total processed images: {len(all_jss_scores)}\")\n",
    "    print(f\"Jensen-Shannon Similarity (JSS): {np.mean(all_jss_scores):.4f} +- {np.std(all_jss_scores):.4f}\")\n",
    "    print(f\"Chi-square Distance (Chi2):     {np.mean(all_chi2_scores):.4f} +- {np.std(all_chi2_scores):.4f}\")\n",
    "    print(f\"Pearson Corr. Coeff. (PCC): {np.mean(all_pcc_scores):.4f} +- {np.std(all_pcc_scores):.4f}\")\n",
    "else:\n",
    "    print(\"Error.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7520e6df",
   "metadata": {},
   "source": [
    "## Sample heatmap visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3746eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "images, labels, _ = next(iter(test_loader))\n",
    "images = images.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "# get patch embeddings (Tensor), shape: (B, num_patches, embed_dim)\n",
    "embeddings = modified_model.patch_embed(images)\n",
    "embeddings = model.patch_embed(images)\n",
    "\n",
    "R, prediction, logits = vision_relevance_propagation(\n",
    "    model = modified_model,\n",
    "    embeddings = embeddings,\n",
    "    targets = labels,\n",
    "    n_classes = 200\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef971f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "attributions = R[0, 1:]  # Shape: [196]\n",
    "num_patches = attributions.shape[0]\n",
    "grid_size = int(np.sqrt(num_patches))\n",
    "\n",
    "\n",
    "if grid_size * grid_size != num_patches:\n",
    "    raise ValueError(f\"Il numero di patch ({num_patches}) non è un quadrato perfetto!\")\n",
    "\n",
    "heatmap = attributions.reshape(grid_size, grid_size) # Shape: [14, 14]\n",
    "\n",
    "heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "\n",
    "heatmap_tensor = torch.tensor(heatmap).detach().cpu()\n",
    "\n",
    "original_image = images[0].detach().cpu()\n",
    "\n",
    "image_to_show = original_image.permute(1, 2, 0).numpy()\n",
    "\n",
    "heatmap_tensor = heatmap_tensor.unsqueeze(0).unsqueeze(0) # Shape: [1, 1, 14, 14]\n",
    "\n",
    "upscaled_heatmap = F.interpolate(\n",
    "    heatmap_tensor,\n",
    "    size=image_to_show.shape[:2], # Prende Altezza e Larghezza (es. [224, 224])\n",
    "    mode='bilinear',\n",
    "    align_corners=False\n",
    ")\n",
    "\n",
    "heatmap_to_show = upscaled_heatmap.squeeze().numpy() # Shape: [224, 224]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax1.imshow(image_to_show)\n",
    "ax1.set_title(\"Immagine Originale\")\n",
    "ax1.axis('off')\n",
    "ax2.imshow(image_to_show)\n",
    "ax2.imshow(\n",
    "    heatmap_to_show,\n",
    "    cmap='jet', # 'jet' o 'viridis' sono buone scelte\n",
    "    alpha=0.5    # Trasparenza per vedere l'immagine sotto\n",
    ")\n",
    "ax2.set_title(\"Spiegazione (Heatmap Attribuzioni)\")\n",
    "ax2.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambaLRPTEST",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
