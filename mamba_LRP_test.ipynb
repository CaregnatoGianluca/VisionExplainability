{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee4b58e6",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torchvision'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m data\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torchvision'"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from torch.utils import data\n",
    "import numpy as np\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer, MambaForCausalLM\n",
    "import torch\n",
    "print(torch.__version__, torch.version.cuda)\n",
    "\n",
    "from models_mamba import vim_base_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_middle_cls_token_div2\n",
    "from mamba_lrp.model.vision_mamba import ModifiedVisionMamba\n",
    "\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd133fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48e7bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ROOT = './CUB/DATASET/CUB_200_2011'\n",
    "DATASET_VALIDATION_RANDOM_SEED = 123\n",
    "BATCH_SIZE = 6\n",
    "DATASET_WORK_NUMBER = 8\n",
    "DATASET_SPLIT_RATIO = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0128ec3c",
   "metadata": {},
   "source": [
    "## Dataset Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f6fdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DatasetLoader.cub import CUB\n",
    "\n",
    "\n",
    "trans_train = transforms.Compose([\n",
    "    # A SCELTA NOSTRA AUGUMENTATION\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # transforms.RandomRotation(30),\n",
    "    \n",
    "    # ATTENZIONE QUA NON DOVREBBE SERIVIRE IL RESIZE (?)\n",
    "    # transforms.RandomResizedCrop(224, scale=(0.7, 1), ratio=(3/4, 4/3)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "trans_test = transforms.Compose([\n",
    "    # ATTENZIONE QUA NON DOVREBBE SERIVIRE IL RESIZE (?)\n",
    "    # transforms.Resize(224),\n",
    "    # transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# create dataset\n",
    "train_data = CUB(DATASET_ROOT, 'train', DATASET_SPLIT_RATIO, DATASET_VALIDATION_RANDOM_SEED, transform=trans_train)\n",
    "valid_data = CUB(DATASET_ROOT, 'valid', DATASET_SPLIT_RATIO, DATASET_VALIDATION_RANDOM_SEED, transform=trans_test)\n",
    "test_data = CUB(DATASET_ROOT, 'test', 0, 0, transform=trans_test)\n",
    "\n",
    "print(\"Train: {}\".format(len(train_data)))\n",
    "print(\"Valid: {}\".format(len(valid_data)))\n",
    "print(\"Test: {}\".format(len(test_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac59b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def imshow(image, label, ax=None, normalize=True):\n",
    "    \"\"\"show single along with label on an ax\"\"\"\n",
    "    \n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "    ax.set_title(label)\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def show_samples(images, labels, nrows=2, ncols=3, title=None, normalize=True):\n",
    "    \"\"\" show multiple samples\n",
    "\n",
    "    args:\n",
    "        nrows (int, optional): number of row\n",
    "        ncols (int, optional): number of column\n",
    "        title (str, optional): title.\n",
    "        normalize (bool, optional): whether the images are normalized\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(nrows, ncols, facecolor='#ffffff', dpi=100)\n",
    "\n",
    "    # .flat: to map samples to multi-dimensional axes\n",
    "    for (ax, image, label) in zip(axes.flat, images, labels):\n",
    "        ax = imshow(image, label, ax, normalize)\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout = True\n",
    "    fig.subplots_adjust(top=0.85, hspace=0.3)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7869e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show samples\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=False)\n",
    "images, labels = next(iter(train_loader))\n",
    "show_samples(images[0:6], labels[0:6], 2, 3, 'SHOW SOME SAMPLES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03edb16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader\n",
    "train_loader = data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, drop_last=False, num_workers=DATASET_WORK_NUMBER)\n",
    "valid_loader = data.DataLoader(valid_data, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=DATASET_WORK_NUMBER)\n",
    "test_loader = data.DataLoader(test_data, batch_size=BATCH_SIZE*2, shuffle=False, num_workers=DATASET_WORK_NUMBER)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e2dd5b",
   "metadata": {},
   "source": [
    "# Model creation (work in progress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa6fe35f",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Mamba.__init__() got an unexpected keyword argument 'bimamba_type'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mvim_base_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_middle_cls_token_div2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m checkpoint_path \u001b[38;5;241m=\u001b[39m hf_hub_download(repo_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhustvl/Vim-base-midclstok\u001b[39m\u001b[38;5;124m\"\u001b[39m, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_model.bin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path)\n",
      "File \u001b[0;32m/media/matteo/SSD/Data/UNI/4year/DeepLearning/mamba/VisionExplainability/VisionExplanability/models_mamba.py:609\u001b[0m, in \u001b[0;36mvim_base_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_middle_cls_token_div2\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;129m@register_model\u001b[39m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mvim_base_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_middle_cls_token_div2\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 609\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mVisionMamba\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m768\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m24\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrms_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresidual_in_fp32\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfused_add_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_pool_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_abs_pos_embed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_rope\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_rope_residual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbimamba_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mv2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_cls_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mif_devide_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_middle_cls_token\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    611\u001b[0m     model\u001b[38;5;241m.\u001b[39mdefault_cfg \u001b[38;5;241m=\u001b[39m _cfg()\n\u001b[1;32m    612\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pretrained:\n",
      "File \u001b[0;32m/media/matteo/SSD/Data/UNI/4year/DeepLearning/mamba/VisionExplainability/VisionExplanability/models_mamba.py:321\u001b[0m, in \u001b[0;36mVisionMamba.__init__\u001b[0;34m(self, img_size, patch_size, stride, depth, embed_dim, d_state, channels, num_classes, ssm_cfg, drop_rate, drop_path_rate, norm_epsilon, rms_norm, initializer_cfg, fused_add_norm, residual_in_fp32, device, dtype, ft_seq_len, pt_hw_seq_len, if_bidirectional, final_pool_type, if_abs_pos_embed, if_rope, if_rope_residual, flip_img_sequences_ratio, if_bimamba, bimamba_type, if_cls_token, if_divide_out, init_layer_scale, use_double_cls_token, use_middle_cls_token, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path \u001b[38;5;241m=\u001b[39m DropPath(drop_path_rate) \u001b[38;5;28;01mif\u001b[39;00m drop_path_rate \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;66;03m# transformer blocks\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[0;32m--> 321\u001b[0m     [\n\u001b[1;32m    322\u001b[0m         create_block(\n\u001b[1;32m    323\u001b[0m             embed_dim,\n\u001b[1;32m    324\u001b[0m             d_state\u001b[38;5;241m=\u001b[39md_state,\n\u001b[1;32m    325\u001b[0m             ssm_cfg\u001b[38;5;241m=\u001b[39mssm_cfg,\n\u001b[1;32m    326\u001b[0m             norm_epsilon\u001b[38;5;241m=\u001b[39mnorm_epsilon,\n\u001b[1;32m    327\u001b[0m             rms_norm\u001b[38;5;241m=\u001b[39mrms_norm,\n\u001b[1;32m    328\u001b[0m             residual_in_fp32\u001b[38;5;241m=\u001b[39mresidual_in_fp32,\n\u001b[1;32m    329\u001b[0m             fused_add_norm\u001b[38;5;241m=\u001b[39mfused_add_norm,\n\u001b[1;32m    330\u001b[0m             layer_idx\u001b[38;5;241m=\u001b[39mi,\n\u001b[1;32m    331\u001b[0m             if_bimamba\u001b[38;5;241m=\u001b[39mif_bimamba,\n\u001b[1;32m    332\u001b[0m             bimamba_type\u001b[38;5;241m=\u001b[39mbimamba_type,\n\u001b[1;32m    333\u001b[0m             drop_path\u001b[38;5;241m=\u001b[39minter_dpr[i],\n\u001b[1;32m    334\u001b[0m             if_divide_out\u001b[38;5;241m=\u001b[39mif_divide_out,\n\u001b[1;32m    335\u001b[0m             init_layer_scale\u001b[38;5;241m=\u001b[39minit_layer_scale,\n\u001b[1;32m    336\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs,\n\u001b[1;32m    337\u001b[0m         )\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth)\n\u001b[1;32m    339\u001b[0m     ]\n\u001b[1;32m    340\u001b[0m )\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# output head\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_f \u001b[38;5;241m=\u001b[39m (nn\u001b[38;5;241m.\u001b[39mLayerNorm \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rms_norm \u001b[38;5;28;01melse\u001b[39;00m RMSNorm)(\n\u001b[1;32m    344\u001b[0m     embed_dim, eps\u001b[38;5;241m=\u001b[39mnorm_epsilon, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs\n\u001b[1;32m    345\u001b[0m )\n",
      "File \u001b[0;32m/media/matteo/SSD/Data/UNI/4year/DeepLearning/mamba/VisionExplainability/VisionExplanability/models_mamba.py:322\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path \u001b[38;5;241m=\u001b[39m DropPath(drop_path_rate) \u001b[38;5;28;01mif\u001b[39;00m drop_path_rate \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;66;03m# transformer blocks\u001b[39;00m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mModuleList(\n\u001b[1;32m    321\u001b[0m     [\n\u001b[0;32m--> 322\u001b[0m         \u001b[43mcreate_block\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m            \u001b[49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m            \u001b[49m\u001b[43md_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43md_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m            \u001b[49m\u001b[43mssm_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mssm_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnorm_epsilon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_epsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrms_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrms_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresidual_in_fp32\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresidual_in_fp32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfused_add_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfused_add_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlayer_idx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[43m            \u001b[49m\u001b[43mif_bimamba\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_bimamba\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbimamba_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbimamba_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minter_dpr\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m            \u001b[49m\u001b[43mif_divide_out\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mif_divide_out\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m            \u001b[49m\u001b[43minit_layer_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minit_layer_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfactory_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth)\n\u001b[1;32m    339\u001b[0m     ]\n\u001b[1;32m    340\u001b[0m )\n\u001b[1;32m    342\u001b[0m \u001b[38;5;66;03m# output head\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_f \u001b[38;5;241m=\u001b[39m (nn\u001b[38;5;241m.\u001b[39mLayerNorm \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rms_norm \u001b[38;5;28;01melse\u001b[39;00m RMSNorm)(\n\u001b[1;32m    344\u001b[0m     embed_dim, eps\u001b[38;5;241m=\u001b[39mnorm_epsilon, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs\n\u001b[1;32m    345\u001b[0m )\n",
      "File \u001b[0;32m/media/matteo/SSD/Data/UNI/4year/DeepLearning/mamba/VisionExplainability/VisionExplanability/models_mamba.py:169\u001b[0m, in \u001b[0;36mcreate_block\u001b[0;34m(d_model, d_state, ssm_cfg, norm_epsilon, drop_path, rms_norm, residual_in_fp32, fused_add_norm, layer_idx, device, dtype, if_bimamba, bimamba_type, if_divide_out, init_layer_scale)\u001b[0m\n\u001b[1;32m    165\u001b[0m mixer_cls \u001b[38;5;241m=\u001b[39m partial(Mamba, d_state\u001b[38;5;241m=\u001b[39md_state, layer_idx\u001b[38;5;241m=\u001b[39mlayer_idx, bimamba_type\u001b[38;5;241m=\u001b[39mbimamba_type, if_divide_out\u001b[38;5;241m=\u001b[39mif_divide_out, init_layer_scale\u001b[38;5;241m=\u001b[39minit_layer_scale, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mssm_cfg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs)\n\u001b[1;32m    166\u001b[0m norm_cls \u001b[38;5;241m=\u001b[39m partial(\n\u001b[1;32m    167\u001b[0m     nn\u001b[38;5;241m.\u001b[39mLayerNorm \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m rms_norm \u001b[38;5;28;01melse\u001b[39;00m RMSNorm, eps\u001b[38;5;241m=\u001b[39mnorm_epsilon, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfactory_kwargs\n\u001b[1;32m    168\u001b[0m )\n\u001b[0;32m--> 169\u001b[0m block \u001b[38;5;241m=\u001b[39m \u001b[43mBlock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m    \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmixer_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfused_add_norm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfused_add_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresidual_in_fp32\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresidual_in_fp32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    177\u001b[0m block\u001b[38;5;241m.\u001b[39mlayer_idx \u001b[38;5;241m=\u001b[39m layer_idx\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m block\n",
      "File \u001b[0;32m/media/matteo/SSD/Data/UNI/4year/DeepLearning/mamba/VisionExplainability/VisionExplanability/models_mamba.py:86\u001b[0m, in \u001b[0;36mBlock.__init__\u001b[0;34m(self, dim, mixer_cls, norm_cls, fused_add_norm, residual_in_fp32, drop_path)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfused_add_norm \u001b[38;5;241m=\u001b[39m fused_add_norm\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# import ipdb; ipdb.set_trace()\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmixer \u001b[38;5;241m=\u001b[39m \u001b[43mmixer_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m norm_cls(dim)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path \u001b[38;5;241m=\u001b[39m DropPath(drop_path) \u001b[38;5;28;01mif\u001b[39;00m drop_path \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m nn\u001b[38;5;241m.\u001b[39mIdentity()\n",
      "\u001b[0;31mTypeError\u001b[0m: Mamba.__init__() got an unexpected keyword argument 'bimamba_type'"
     ]
    }
   ],
   "source": [
    "model = vim_base_patch16_224_bimambav2_final_pool_mean_abs_pos_embed_with_middle_cls_token_div2(pretrained=False)\n",
    "\n",
    "checkpoint_path = hf_hub_download(repo_id=\"hustvl/Vim-base-midclstok\", filename=\"pytorch_model.bin\")\n",
    "state = torch.load(checkpoint_path)\n",
    "\n",
    "modified_model = ModifiedVisionMamba(model, zero_bias=True)\n",
    "modified_model.eval()\n",
    "model.backbone.embeddings.requires_grad = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mambaLRP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
